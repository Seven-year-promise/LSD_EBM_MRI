num_epochs 201
batch_size 4
ebm_lr 0.0001
gen_lr 0.0001
load_from_ep None
epoch_start 0
recon_loss bce
ebm_dyn_lr None
gen_dyn_lr None
EBM_reg 0
save_model True
train_set XXX_bodies_data_train
validation_set XXX_bodies_data_validation
test_set XXX_bodies_data_test
experiment Test
save_visu True
ep_betw_val 1
num_channels 16
num_latents 100
prior_var 1.0
prior_bn True
gen_var 0.3
ebm_num_layers 3
ebm_inner_dim 300
use_samp_buff False
samp_buff_size 256
pr_mcmc_steps_tr 20
pr_mcmc_steps_val 20
pr_mcmc_step_size 0.4
pr_mcmc_noise_var 1.0
po_mcmc_steps_tr 20
po_mcmc_steps_val 20
po_mcmc_steps_test 20
po_mcmc_step_size 0.1
po_mcmc_noise_var 1.0
pr_mcmc_step_gamma 1.0
po_mcmc_step_gamma 1.0
test_perf True
num_channel 16
num_latent 100
/itet-stor/yankwang/net_scratch/conda_envs/ve_py37_lsd_A100_20230202/lib/python3.7/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.
  "the returned array has changed.", UserWarning)
main_LEBM_Vert_04.py:37: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  dice_score = dc(pred.astype(np.int),gt.astype(np.int))
Traceback (most recent call last):
  File "main_LEBM_Vert_04.py", line 375, in <module>
    en_noise = e_func.forward(z_e_0).mean()
  File "/usr/itetnas04/data-scratch-01/yankwang/data/Codes/MT_LSD_EBM_ToKyr/LSDEBM_Vert/LSD-EBM_code/model_Unet_lebm.py", line 271, in forward
    z = self.ebm(z)
  File "/itet-stor/yankwang/net_scratch/conda_envs/ve_py37_lsd_A100_20230202/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/itet-stor/yankwang/net_scratch/conda_envs/ve_py37_lsd_A100_20230202/lib/python3.7/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/itet-stor/yankwang/net_scratch/conda_envs/ve_py37_lsd_A100_20230202/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/itet-stor/yankwang/net_scratch/conda_envs/ve_py37_lsd_A100_20230202/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 96, in forward
    return F.linear(input, self.weight, self.bias)
  File "/itet-stor/yankwang/net_scratch/conda_envs/ve_py37_lsd_A100_20230202/lib/python3.7/site-packages/torch/nn/functional.py", line 1847, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`
